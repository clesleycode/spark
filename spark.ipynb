{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This guide was written in Python 3.5.\n",
    "\n",
    "\n",
    "### Python and Pip\n",
    "\n",
    "Download [Python](https://www.python.org/downloads/) and [Pip](https://pip.pypa.io/en/stable/installing/).\n",
    "\n",
    "### Modules\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "### Other \n",
    "\n",
    "Follow [these](https://www.dataquest.io/blog/pyspark-installation-guide/) instructions for PySpark setup! \n",
    "\n",
    "\n",
    "### \n",
    "\n",
    "install.packages(\"sparklyr\")\n",
    "\n",
    "library(sparklyr)\n",
    "\n",
    "spark_install()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Spark is a low-level system for distributed computation on clusters, capable of doing in-memory caching between stages, improving the performance. This is in contrast to Hadoop, which instead writes everything to disk. Spark is also a much more flexible system in that it's not only constrained to MapReduce. \n",
    "\n",
    "Now, it might seem as though Spark is a replacement for Hadoop; and though it sometimes is used as a replacement, it can also be used to complement Hadoop's functionality. By running Spark on top of a Hadoop cluster, you can still leverage HDFS and YARN and then have Spark replace MapReduce. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REPLs\n",
    "\n",
    "Spark is composed of a built-in **R**ead-**E**valuate-**P**rint-**L**oop in the form of a shell that can be used for interactive analysis. You can begin by entering the following into your terminal:\n",
    "\n",
    "`$SPARK_HOME/bin/pyspark`\n",
    "\n",
    "Next, you can run the following in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3c1e3c027651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"demo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(\"local[*]\", \"demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Spark only allows one Spark Context to be active at a time,  so you'll need to stop the current spark context before starting a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this doesn't work, make sure the `PYTHONPATH` contains the module:\n",
    "\n",
    "``` bash\n",
    "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/:$PYTHONPATH\n",
    "```\n",
    "\n",
    "Now in R, the process is very similar: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(SparkR, \n",
    "        lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))\n",
    "sparkR.session(master = \"local[*]\", \n",
    "               sparkConfig = list(spark.driver.memory = \"2g\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins in Spark\n",
    "\n",
    "In Spark, a join is executed by emitting key-value pairs `(k, v)` and joining on similar keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames in Spark\n",
    "\n",
    "Spark SQL provides support for writing SQL queries in Spark. The key feature of Spark SQL is the use of DataFrames instead of RDDs. Recall that a DataFrame is data organized into columns and rows \n",
    "\n",
    "In Spark, operations on DataFrames are essentially operations on RDDs, but the RDDs are created by the execution engine and not [directly] by the user. With Spark, it's also possible to convert RDDs to DataFrames and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Spark\n",
    "\n",
    "There are two machine learning APIs in Spark, `mllib` and `ml`. `mllib` is based on RDDs while the `ml` package is based on DataFrames.\n",
    "\n",
    "### Vectors \n",
    "\n",
    "A local vector is often used as a base type for RDDs in `mllib`. A local vector has integer-typed indices and double-typed values that is stored on a single machine. `mllib` supports two types of local vectors: dense and sparse. \n",
    "\n",
    "A dense vector is backed by a double array representing its entry values, whereas a sparse vector is backed by two parallel arrays: indices and values. For dense vectors, `mllib` uses either Python lists or NumPy arrays. For sparse vectors, users can construct a SparseVector object from `mllib` or use SciPy scipy.sparse column vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparklyr\n",
    "\n",
    "R's Spark Interface. If you know dplyr, you know most of sparklyr. It's fairly new though. Scala and Python are more developed. \n",
    "\n",
    "Converts code to SQL before passing to spark. \n",
    "\n",
    "Workflow: \n",
    "1. Connect w spark_connect(\"local\")\n",
    "2. Do work\n",
    "3. Close connection w spark_disconnect(sc=SPARKCONNECTION)\n",
    "\n",
    "spark_connect() -- takes URL that gives location to spark\n",
    "spark_version(sc=SPARKCONNECTION)\n",
    "\n",
    "spark_read_csv() -- Allows you to read CSVs into Spark. \n",
    "\n",
    "dplyr.copy_to(dest=SPARKCONNECTION, df=DATAFRAME) -- copy data from R to Spark, VERY SLOW.\n",
    "\n",
    "src_tbls(x=SPARKCONNECTION) -- lists all dataframes stores in Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dplyr\n",
    "library(dplyr)\n",
    "\n",
    "# Explore track_metadata structure\n",
    "str(track_metadata)\n",
    "\n",
    "# Connect to your Spark cluster\n",
    "spark_conn <- spark_connect(\"local\")\n",
    "\n",
    "# Copy track_metadata to Spark\n",
    "track_metadata_tbl <- copy_to(dest=spark_conn, df=track_metadata)\n",
    "\n",
    "# List the data frames available in Spark\n",
    "src_tbls(x=spark_conn)\n",
    "\n",
    "# Disconnect from Spark\n",
    "spark_disconnect(spark_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
